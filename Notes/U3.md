

# <u>L11. The Memory Hierarchy</u>

## Storage technologies and trends

**<u>The speed gap between CPU, memory and mass storage continues to widen.</u>**

## Locality of reference

**<u>Well-written programs exhibit a property called locality.</u>**

- Principle of Locality: 
	- Programs tend to use data and instructions with addresses near or equal to those they have used recently
- Temporal locality:  
	- Recently referenced items are likely  to be referenced again in the near future
- Spatial locality:  
	- Items with nearby addresses tend  to be referenced close together in time

### Qualitative Estimates of Locality

- Claim: Being able to look at code and get a qualitative sense of its locality is a key skill for a professional programmer.

## Caching in the memory hierarchy

**<u>Memory hierarchies based on caching close the gap by exploiting locality.</u>**

### Example Memory  Hierarchy

<img src="/Users/tian/Library/Mobile Documents/com~apple~CloudDocs/Typora/15-213/U.assets/image-20210715104350765.png" alt="image-20210715104350765" style="zoom: 33%;" />

### Caches

- Cache: A smaller, faster storage device that acts as a staging area for a subset of the data in a larger, slower device.
- Fundamental idea of a memory hierarchy:
	- For each k, the faster, smaller device at level k serves as a cache for the larger, slower device at level k+1.
- Why do memory hierarchies work?
	- Because of locality, programs tend to access the data at level k more often than they access the data at level k+1. 

### Types of Cache Misses

- Cold (compulsory) miss
- Conflict miss
- Capacity miss

## *Thinking*

这一讲介绍了计算机存储的技术，从而引出了最主要的问题，快速的贵，便宜的慢。

由于Locality，可以使用cashing来优化这个问题。

Locality分为Temporal和Spatial，后者需要代码的技巧，如2D array中按存储中的顺序（行还是列优先）访问。



# <u>L12. Cache Memories</u>

## Cache memory organization and operation

<img src="/Users/tian/Library/Mobile Documents/com~apple~CloudDocs/Typora/15-213/U.assets/image-20210715131107900.png" alt="image-20210715131107900" style="zoom:50%;" />

### Writes

- Multiple copies of data exist:
	- L1, L2, L3, Main Memory, Disk
- What to do on a write-hit?
	- Write-through (write immediately to memory)
	- Write-back (defer write to memory until replacement of line)
		- Need a dirty bit (line different from memory or not)
- What to do on a write-miss?
	- Write-allocate (load into cache, update line in cache)
		- Good if more writes to the location follow
	- No-write-allocate (writes straight to memory, does not load into cache)
- Typical
	- Write-through + No-write-allocate
	- Write-back + Write-allocate

### Intel Core i7 Cache Hierarchy

<img src="/Users/tian/Library/Mobile Documents/com~apple~CloudDocs/Typora/15-213/U.assets/image-20210715131313306.png" alt="image-20210715131313306" style="zoom:50%;" />

### Writing Cache Friendly Code

- Make the common case go fast
	- Focus on the inner loops of the core functions
- Minimize the misses in the inner loops
	- Repeated references to variables are good (temporal locality)
	- Stride-1 reference patterns are good (spatial locality)

## Performance impact of caches

### The memory mountain

- Read throughput (read bandwidth)
	- Number of bytes read from memory per second (MB/s)

<img src="/Users/tian/Library/Mobile Documents/com~apple~CloudDocs/Typora/15-213/U.assets/Screen Shot 2021-07-15 at 13.49.44.png" alt="Screen Shot 2021-07-15 at 13.49.44" style="zoom:50%;" />

### Rearranging loops to improve spatial locality

- Matrix Multiplication (kij)

<img src="/Users/tian/Library/Mobile Documents/com~apple~CloudDocs/Typora/15-213/U.assets/image-20210715131908431.png" alt="image-20210715131908431" style="zoom:50%;" />

### Using blocking to improve temporal locality

- Example: Matrix Multiplication
- Assume: 
	- Matrix elements are doubles
	- Cache block = 8 doubles
	- Cache size C << n (much smaller than n)

- Problem
	- each element is visited n times and each time we visit it, it's no longer in the cache(cache size limitation)
- Solution
	- Three blocks       fit into cache: $3B^2 < C$
	- during each calculation of one block, data needs to be loaded once
- Miss Analysis Comparison
	- No blocking: (9/8) * n3
	- Blocking: 1/(4B) * n3

## *Thinking*

本讲介绍了基于SRAM的Cache的组织和操作方式。基于此分析了Stride（Spatial Locality）和Input Size（Temporal Locality）对于CPU读取速度的影响（Memory Mountain）。

进而介绍了优化两种Locality的方法：

1. Spatial：对于多层循环顺序的调整，进而使得最内层的Miss Rate降到最低
2. Temporal：将大数据的重复逐元素操作（每个数据需要重复利用）改为小于cache size的块操作，从而让每个元素多次读取的时候尽量多的直接从cache中读取



# <u>Lab4. Cachelab</u>

## *Thinking*

第一阶段写了一个cache simulator，模拟cpu中缓存，从而更了解其架构和运行方式。第一个C语言的程序，挺有意思的：

- 有更多的针对底层硬件的操作，如内存中的heap（堆）区进行动态内存分配（dynamic memory allocation），如bit level operation（位元层级操作）等等。
- 因为可以直接编译为汇编语言，因此所有数据类型和函数都有机器语言的相对准确的对应方式，因此可以直接思考机器层面的效率。
- 特别的数据类型，如struct、enum等等（struct只保存数据，不保存方法，因此距oop只有一步之遥?）
- 指针类型，存储地址而非数据，因此函数调用时会改变本身的数据
	- 函数需要改变原始函数，原始数据不是指针时，传入函数只需加 `&` 即可
	- 指针类型如果为struct，调用子数据时使用 `->`
- functional programming：定义数据类型，定义函数，运行，即为functional programming的方法。
	- ++或许可以这么思考？每个c文件就是一个对象，main函数即为其总的定义加上实现的功能。

第二阶段是要优化一个矩阵转置函数，从而有最少的缓存miss。这里边有几个重点：

- Blocking，通过这个方法让同一行的元素在block size内的访问尽量只有一次miss
- 由于是direct-mapped cache，set index相同时必然要eviction，是miss的主要来源
	- 对于方矩阵，由于A和B的首元素是对齐的（aligned），因此两个矩阵中对应元素的set index都相同，但由于是转置复制，只有对角线上的Blocking受这个问题的影响。
		- if Blocking size <= block size
			- 转置对角线上的Blocking时，读取A的每一行时，将对角线元素先保存，待该行其他元素转置完后，再将保存的该元素复制给B。这样A的miss次数只有Blocking size，而B由于第一行外每行都被A evict过一次，因此miss次数是2*Blocking size-1
			- 对于不再对角线上的Blocking，A和B的miss次数都为Blocking size
		- else，分析是一样的，计算更复杂一些
	- cache能保存的整数的总数量为block size * amount of sets = 8 * 32 = 256。因此如果row size是256的因子的话，在（256/row size）行之后，行的每个元素的set index就会重复，这是32\*32和64\*64两个情况下Blocking size的主要限制（256/32和256/64）。
		- 可以通过设置temp临时保存数据来减少miss的次数，但由于是针对 特殊情况的优化，此处不进行讨论，虽然通过进一步的分析，这种方式应该可以被泛化（generalized）
	- 如果row size和column size（实际上是B的row size）都不是256的因子，则可以使用最大Blocking size：256/2=128（e.g. 16\*8）。由于block size就是8，因此直接用16\*16也是一样的。

































